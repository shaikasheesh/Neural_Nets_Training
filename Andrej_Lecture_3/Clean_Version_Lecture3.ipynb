{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9045ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abadc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt','r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fb94a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is: 27\n"
     ]
    }
   ],
   "source": [
    "chars = ['.'] + sorted(set(''.join(words))) # '.' represents start and end token for a word\n",
    "vocab_size = len(chars)\n",
    "print('vocabulary size is:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbcf833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder and decoder\n",
    "stoi = {j:i for i,j in enumerate(chars)}\n",
    "itos = {i:j for i,j in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa9182",
   "metadata": {},
   "source": [
    "# A) Steps to Follow to create MLP NN which will Generate Names\n",
    "\n",
    "1) Create Train, Dev & Test Split for the names dataset\n",
    "\n",
    "2) Intialization of Neural Network Parameters\n",
    "\n",
    "3) Train the Neural Network\n",
    "\n",
    "4) Check the Dev Loss\n",
    "\n",
    "5) Only Once you feel the model is a good fit, Check the Test loss\n",
    "\n",
    "# B) Sampling\n",
    "\n",
    "1) Initialize the Context with [0,0,0] --> ...\n",
    "\n",
    "2) get the embeddings of the Context\n",
    "\n",
    "3) calculate the logits & probabilities\n",
    "\n",
    "4) sample from the probabilities \n",
    "\n",
    "5) convert the index of sample to string to get the character using the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a3775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# 1) Create Train, Dev & Test Split for the names dataset\n",
    "\n",
    "block_size = 3 \n",
    "def build_dataset(words):\n",
    "    x,y = [],[]\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            x.append(context)\n",
    "            ix = stoi[ch]\n",
    "            y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(x)\n",
    "    Y = torch.tensor(y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,Ytr = build_dataset(words[:n1])\n",
    "Xdev,Ydev = build_dataset(words[n1:n2])\n",
    "Xte,Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68dc5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Intialization of Neural Network Parameters\n",
    "\n",
    "n_dim = 10\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size,n_dim))\n",
    "W1 = torch.randn((n_dim * block_size, 200))\n",
    "b1 = torch.randn(200)\n",
    "W2 = torch.randn((200,vocab_size))\n",
    "b2 = torch.randn(vocab_size)\n",
    "\n",
    "parameters = [C,W1,b1,W2,b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "254fb96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss is: 2.482802629470825\n"
     ]
    }
   ],
   "source": [
    "# 3) Train the Neural Network\n",
    "loop_size = 80000\n",
    "batch_size = 50\n",
    "\n",
    "for i in range(loop_size):\n",
    "    ix = torch.randint(0,Xtr.shape[0],(batch_size,))\n",
    "    emb = C[Xtr[ix]]\n",
    "    h = torch.tanh(emb.view(-1,(n_dim * block_size)) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,Ytr[ix])\n",
    "\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #updation\n",
    "    lr = 0.1 if i <10000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "print('Training Loss is:',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07a16e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss is: 2.1865222454071045\n"
     ]
    }
   ],
   "source": [
    "# 4) Check the Dev Loss \n",
    "\n",
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(-1,(n_dim * block_size)) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits,Ydev)\n",
    "\n",
    "print('Dev Loss is:',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74c5becc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Loss is: 2.198982000350952\n"
     ]
    }
   ],
   "source": [
    "# 5) Check the Test Loss \n",
    "\n",
    "emb = C[Xte]\n",
    "h = torch.tanh(emb.view(-1,(n_dim * block_size)) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits,Yte)\n",
    "\n",
    "print('Dev Loss is:',loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b527078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emeerael\n",
      "amnileh\n",
      "salenni\n",
      "ifhadne\n",
      "heisa\n",
      "josans\n",
      "genai\n",
      "navian\n",
      "ilah\n",
      "ynonaysen\n",
      "elessivalyussan\n",
      "odel\n",
      "mana\n",
      "atholexmenie\n",
      "naiah\n",
      "anenleylin\n",
      "domia\n",
      "teni\n",
      "belty\n",
      "jayia\n"
     ]
    }
   ],
   "source": [
    "#Sampling\n",
    "g = torch.Generator().manual_seed(2147483647 + 1)\n",
    "\n",
    "for i in range(20):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        h = torch.tanh(emb.view(-1,(n_dim * block_size)) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        ix = torch.multinomial(probs, num_samples = 1, replacement = True, generator = g).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "    res = ''.join([itos[i] for i in out])\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
